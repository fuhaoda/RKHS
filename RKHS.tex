\documentclass[notes=no]{beamer}
%\documentclass[notes=only]{beamer}
%\documentclass[notes]{beamer}

\usetheme[white, compactlogo]{Lilly}





% begin definition
\def\conas{\stackrel{a.s.} {\rightarrow}}         % conv. a.s.
\def\conP{\stackrel{\cal P} {\rightarrow}}        % conv. in probability
\def\conD{\stackrel{\cal D} {\rightsquigarrow}}        % conv. in distribution
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}
\def\iid{\stackrel{ iid} {\sim}}          % i.i.d
\def\hat{\widehat}
\def\tilde{\widetilde}

%general sets
\def\cal{\mathcal}
\def\calA{{\cal A}} %Action sets
\def\calB{{\cal B}} %Bounded set
\def\calC{{\cal C}} %Convex set
\def\calH{{\cal H}} %Hilbert Space
\def\calS{{\cal S}} %A regular set
\def\calNr{{\cal N}_r} %Neighborhood
\def\calX{{\cal X}} %Covariate space
\def\calF{{\cal F}} %Function space

\def\calD{{\cal D}} %decision rule


%special sets
\def\bbR{{\mathbb{R}}} %Real number
\def\bbN{{\mathbb{N}}}%Natural number
\def\bbQ{{\mathbb{Q}}} %Rational number
\def\bbZ{{\mathbb{Z}}} %Integer

%Matrix
\def\bA{{\bf A}}
\def\bB{{\bf B}}
\def\bC{{\bf C}}
\def\bD{{\bf D}}
\def\bH{{\bf H}}
\def\bI{{\bf I}}
\def\bJ{{\bf J}}
\def\bP{{\bf P}}
\def\bT{{\bf T}}
\def\bW{{\bf W}}
\def\bX{{\bf X}}

%vector
\def\balpha{{\boldsymbol \alpha}}
\def\bbeta{{\boldsymbol \beta}}
\def\btheta{{\boldsymbol \theta}}

\def\bzero{{\bf 0}}
\def\bone{{\bf 1}}
\def\bbf{{\bf f}}
\def\br{{\bf r}}
\def\by{{\bf y}}

%notations
\def\sign{{\mathrm{sign}}}
\def\var{{\mathrm{var}}}
\def\cov{{\mathrm{cov}}}
\def\ind{\perp\!\!\!\perp}

%others
\def\mif{\mathrm{if}\ }
\def\ow{\mathrm{otherwise}\ }
\def\st{\mathrm{subject\ to}\quad }
\def\diag{\mathrm{diag}}
\def\minimize{\mathrm{minimize}\quad }
\def\maximize{\mathrm{maximize}\quad }
\def\dom{{\rm dom}}



\title{Introduction of Reproducing Kernel Hilbert Space with An Application to Personalized Intervention System}    % Enter your title between curly braces
\author{Drafted by Haoda Fu, Ph.D.}                 % Enter your name between curly braces
\institute{Eli Lilly and Company}      % Enter your institute name between curly braces
\date{\today}

\begin{document}
\begin{frame}
	\frametitle{Why}
		\begin{variableblock}{RKHS}{bg=LillyGreen25,fg=black}{bg=LillyGreen75,fg=black}
			RKHS theory is one of the most useful and elegant theories for mathematics and machine learning. It quantifies the model complexity by pointing out that the optimal solution from an infinity functional space is embedded into a finite dimensional subspace.
		\end{variableblock}
	\pause
		\begin{variableblock}{Personalized Intervention and Digital Health}{bg=LillyGreen25,fg=black}{bg=LillyGreen75,fg=black}
			 \begin{description}
				\item[Context:] Digital technology will enable us to collect more individual patient data.
				\item[Decision:] The purpose of collecting these data is to generate actionable insights. 
				\item[Reward:] The goal of these actionable insights is to maximize individual patient's outcomes.
			\end{description}	
	\end{variableblock}
\end{frame}

\begin{frame}
	\frametitle{Why}
	Can we leverage RKHS theory to enhance the solution for personalized intervention algorithms?
\end{frame}	
	

\section{Introduction of Reproducing Kernel Hilbert Spaces}
\begin{frame}
	\tableofcontents[currentsection]
\end{frame}
\begin{frame}
	\frametitle{Introduction of RKHS}
	\begin{variableblock}{Logic review}{bg=LillyGreen25,fg=black}{bg=LillyGreen75,fg=black}
		\begin{eqnarray*}
			&\mbox{Field}& \\
			&\Downarrow& \\
			&\mbox{Vector Spaces}& \\
			&\Downarrow& \\
			&\mbox{Banach Spaces}& \\
			&\Downarrow& \\
			&\mbox{Hilbert Spaces}& \\
			&\Downarrow& \\
			&\mbox{Reproducing Kernel Hilbert Spaces}&
		\end{eqnarray*}
	\end{variableblock}
\end{frame}


\begin{frame}
	\frametitle{Field: A field $F$ is a structure $(F,+,\cdot,0,1 )$}
	The following properties hold for a field:
	\begin{enumerate}
		\item $\forall \alpha, \beta \in F, \alpha + \beta = \beta +\alpha$ (commutativity of addition)
		\item $\forall \alpha, \beta, \gamma \in F, \alpha + (\beta + \gamma) = (\alpha+\beta) + \gamma$ (commutativity of addition)
		\item $\forall \alpha, \beta \in F, \alpha \cdot \beta = \beta \cdot \alpha$ (commutativity of multiplication)
		\item $\forall \alpha, \beta, \gamma \in F, \alpha \cdot (\beta \cdot \gamma) = (\alpha\cdot\beta) \cdot \gamma$ (commutativity of multiplication)
		\item $\forall \alpha, \beta, \gamma \in F, \alpha \cdot (\beta + \gamma)= \alpha\cdot\beta + \alpha\cdot\gamma$ (distributivity)
		\item $\exists 0 \in F, s.t. \forall \alpha \in F, 0 + \alpha = \alpha$ (additive identity)
		\item $\forall \alpha \in F, \exists -\alpha \in F, s.t. \alpha+(-\alpha)=0$ (additive inverse)
		\item $\exists 1 \in F, s.t. \forall \alpha \in F, 1 \cdot \alpha = \alpha$ (multiplication identity)
		\item $\forall \alpha \in F, \alpha \neq 0, \exists \alpha^{-1} \in F, s.t. \; \alpha\cdot\alpha^{-1}=1$ (multiplicative inverse)
	\end{enumerate}
	Example: $\mathbb{Q}$.
\end{frame}


\begin{frame}
	\frametitle{Vector Spaces: $+: V \times V \mapsto V, \cdot: F \times V \mapsto V$}
	The following properties hold for a vector space:
	\begin{enumerate}
		\item $\forall f, g \in V, f + g = g + f$ (commutativity of addition)
		\item $\forall f, g, h \in V, f + (g + h) = (f+g) + h$ (commutativity of addition)
		\item $\exists 0 \in V, s.t. \forall f \in V, 0 + f = f$ (additive identity)
		\item $\forall f \in V, \exists -f \in V, s.t. \; f+(-f)=0$ (additive inverse)
		\item $\forall f \in V, \forall \alpha, \beta \in F, \alpha\cdot(\beta \cdot f) = (\alpha\cdot\beta)\cdot f$ (associativity of multiplication)
		\item $\forall f \in V, \forall \alpha, \beta \in F, (\alpha + \beta) \cdot f = \alpha \cdot f + \beta \cdot f$ (scalar distributivity)
		\item $\forall f, g \in V, \forall \alpha \in F, \alpha \cdot ( f+g) = \alpha \cdot f + \alpha \cdot g$ (vector distributivity)
		\item $\forall f \in V, \exists 1 \in F, 1 \cdot f = f$ (multiplicative identity)
	\end{enumerate}
	Example: $\mathbb{R}, \mathbb{R}^2, \mathbb{R}^{\mathbb{R}}, \mathbb{R}^{\cal X}, C(\cal{X})$.
\end{frame}

\note[enumerate]{
\item \bbR^\bbR: the set of functions from $\bbR \rightarrow \bbR$.
\item \bbR^{\calX}: the set of functions from $\calX \rightarrow \bbR$
}


\begin{frame}
	\frametitle{Banach Spaces}
	\begin{definition}[Banach Space]
		A complete normed vector space is called a Banach Space.
	\end{definition}
	\begin{variableblock}{Normed Spaces}{bg=LillyGreen25,fg=black}{bg=LillyGreen75,fg=black}
		A vector space $V$ is called a normed space if $\forall f \in V$, $\exists \|f\| \in \mathbb{R}$, called the norm of $u$, such that
		\begin{itemize}
			\item $\|f\| > 0$ if $f \neq 0$, and $\|f\| = 0$ implies $f=0$
			\item $\|f+g\| \le \|f\| + \|g\|$
			\item $\|\alpha f \| = |\alpha| \|f\|$, where $\alpha \in \mathbb{R}$
		\end{itemize}
	\end{variableblock}
	\begin{variableblock}{Complete Vector Spaces}{bg=LillyGreen25,fg=black}{bg=LillyGreen75,fg=black}
		A vector space $V$ is called complete if every Cauchy sequence in $V$ convergent.
	\end{variableblock}
	Example: Euclidean Spaces in $\mathbb{R}^n$, $L^p(V,{\cal F}, \mu)$ where $p \in [1,+\infty) -\{2\}$.
\end{frame}

\note{
$L^2$ space is a banach space, but it is also a Hilbert space. $V$ is the vector space, $\calF$ is the $\sigma$-measure, and $\mu$ is the measure. Because the Banach space is defined by integal.}


\begin{frame}
	\frametitle{Hilbert Spaces}
	\begin{definition}[Hilbert Spaces]
		A Hilbert space is a Banach space equipped with a dot product $\langle \cdot, \cdot \rangle$. The dot product operation satisfies the following properties, $\forall f, g, h \in \cal{H}$ and $\alpha \in F$
		\begin{itemize}
			\item Commutative: $\langle f, g\rangle = \langle g , f \rangle$ and $\langle f,g\rangle \in F$.
			\item Associative: $\langle \alpha \cdot f,  g\rangle = a \cdot \langle f , g\rangle$
			\item Distributive: $\langle f, g+h\rangle = \langle f, g\rangle + \langle f, h\rangle$
		\end{itemize}
		The norm of $f$ is defined as $\|f\| = \sqrt{\langle f,f\rangle}$.
	\end{definition}
	Example: $L^2(a,b)$.
\end{frame}


\begin{frame}
	\frametitle{Reproducing Kernel Hilbert Spaces(RKHS)}
	\begin{definition}[RKHS]
		A Hilbert space of real-valued functions $\cal{H}$ is an RKHS if every evaluation functional is bounded and continuous.
	\end{definition}
	\begin{variableblock}{Evaluation functional}{bg=LillyGreen25,fg=black}{bg=LillyGreen75,fg=black}
		Let $\cal{H}$ be a Hilbert space of real-valued functions from $\cal{X}$ to $\mathbb{R}$, i.e. $\mathbb{R}^{\cal{X}}$  where $\cal{X}$ is an arbitrary set. For a fixed $x \in {\cal X}$, the evaluation functional ${\cal L}_x:\cal{H} \rightarrow \mathbb{R}$ is defined as,
		\begin{align*}
			{\cal L}_x (f) &\triangleq f(x),
		\end{align*}
		where $f \in \cal{H}$. Evaluation functionals are linear since,
		\begin{eqnarray*}
			{\cal L}_x (\alpha \cdot f + \beta \cdot g) = \alpha \cdot f(x) + \beta \cdot g(x) = \alpha \cdot {\cal L}_x f + \beta \cdot {\cal L}_x g.
		\end{eqnarray*}
	\end{variableblock}
\end{frame}

\begin{frame}
	\frametitle{Reproducing Kernel}
	\begin{theorem}[Riesz representation theorem]
		If $T$ be a bounded linear functional on a Hilbert space ${\cal H}$,  then there exists a unique $g \in {\cal H}$ such that $T(f) = \langle g, f\rangle, \forall  f \in {\cal H}$. The element $g$ is called the representer of $T$.
	\end{theorem}
	\begin{definition}[Reproducing kernel]
		Let $K_x \in {\cal H}$ be a representer, and $\langle K_x, f\rangle = f(x), \forall f \in {\cal H}$. The symmetric bivariate function $K(x,y)=\langle K_x, K_y\rangle=K_x(y)=K_y(x)$ is called the reproducing kernel (RK) of the space ${\cal H}$ and it has the reproducing property $\langle K(x,\cdot), f(\cdot)\rangle = f(x)$.
	\end{definition}
	
	\begin{theorem}[Moore-Aronszajn theorem]
		for every positive definite function $K\langle\cdot, \cdot\rangle$ on ${\cal X} \times {\cal X}$ , there exists a unique RKHS and vice versa.
	\end{theorem}
\end{frame}

\note{
Riesz theorem can be proved by construction. For seperable Hilbert space, we can have basis as $\phi_j$, we can see that 
\begin{align*}
g&=\sum_j T(\phi_j) \phi_j. 
\end{align*}
For each evaluation functionals ($\calL_x$), we have
\begin{align*}
\calL_x(f) &=f(x)=\langle f, k_x \rangle
\end{align*}

The proof of the Moore-Aronszajn theorem goes to 4 steps
\begin{enumerate}
	\item Define a pre-Hilbert space $\calH_0$.
	\item define a dot product on $\calH_0$.
	\item construct $\calH$ as a completion of $\calH_0$.
	\item define a dot product on $\calH$.
\end{enumerate}
}

\begin{frame}
	\frametitle{Why RKHS Matters - From Infinity to Finite}
	A general class of regularization problems has the form,
	\begin{eqnarray*}
		\underset{f \in {\cal H}} {\minimize}  \left[\sum_{i=1}^n L\{y_i, f(x_i)\} + \lambda J(f) \right],
	\end{eqnarray*}
	where $L(\cdot, \cdot)$ is a loss function, $J(\cdot)$ is a penalty function, and ${\cal H}$ is a space of functions on which $J(\cdot)$ is defined. In particular, we have ${\cal H} = {\cal H}_0\oplus{\cal H}_1$, with the null space ${\cal H}_0$ consisting of, for example, low degree polynomials in that elements do not get penalized. The penalty becomes $J(f)=\|P_1f\|^2$, where $P_1$ is the orthogonal projection of $f$ onto ${\cal H}_1$. If ${\cal H}_1$ is a RKHS and $L(\cdot, \cdot)$ is continuous and convex, the solution has the form,
	\begin{align*}
		f(x) &= \sum_{j=1}^m \beta_j h_j(x) + \sum_{i=1}^n \alpha_i K(x,x_i),
	\end{align*}
	where the first term represents an expansion in ${\cal H}_0$.
\end{frame}




\begin{frame}
	\frametitle{Existence of Minimizer}
	A functional $F(f)$ on a Hilbert space ${\cal H}$ is said to be \textbf{convex} if $\forall f,g \in {\cal H}, F\{\alpha\cdot f +(1-\alpha)\cdot g\} \le \alpha F(f)+(1-\alpha)F(g), \forall \alpha \in (0,1)$, and the convexity is strict if the equality holds only for $f=g$.
	
	\begin{theorem}[Existence]
		Suppose $L(f)$ is a continuous and convex functional in a Hilbert space ${\cal H}$ and $J(f)$ is a square (semi) norm in ${\cal H}$ with a null space ${\cal H}_0$ of finite dimension. If $L(f)$ has a unique minimizer in ${\cal H}_0$, then $L(f)+\lambda J(f)$ has a minimizer in ${\cal H}$.
	\end{theorem}
	Key to proof: Since $L(f)+\lambda J(f)$ must has a minimizer in a closed rectangle set,
	\begin{eqnarray*}
		R_{\rho,\gamma} &=& \{f: f \in {\cal H}, J(f) \le \rho, \|f-f_0\|_0 \le \gamma \},  \forall \rho, \gamma, 
	\end{eqnarray*}
	where $f_0$ is the unique minimizer in ${\cal H}_0$, and $\|\cdot\|_0$ is the norm on ${\cal H}_0$. If $L(f)+\lambda J(f)$ doesn't have a minimizer in ${\cal H}$, the minimizer must be on the boundary of $R_{\rho,\gamma}$, by convexity,  which cannot be true $\forall \rho, \gamma$.
\end{frame}


\begin{frame}
	\frametitle{The Representer Theorem}
	\begin{theorem}[The representer theorem]
		Suppose $L(f)$ is a continuous and convex functional in a Hilbert space ${\cal H}$ and $J(f)$ is a square (semi) norm in ${\cal H}$ with a null space ${\cal H}_0$ of finite dimension. If $L(f)$ has a unique minimizer in ${\cal H}_0$, then $L(f)+\lambda J(f)$ has a minimizer in ${\cal H}$, and
		\begin{align*}
			f(x) &= \sum_{j=1}^m \beta_j h_j(x) + \sum_{i=1}^n \alpha_i K(x,x_i).
		\end{align*}
	
	\end{theorem}
	Key to proof: $f=f_0+f_K + f_{\eta}$ where $f_0 \in {\cal H}_0$, $f_K \in {\cal H}_K \triangleq\mbox{span}\{K(x_1,\cdot), i=1,c\dots,n\}$ and $f_\eta \in {\cal H}\ominus({\cal H}_K\oplus{\cal H}_0)$. Then, we have $0=\langle K(x_i,\cdot), f_\eta(\cdot)\rangle=f_\eta(x_i)$. So, $L\{(f_0+f_K + f_{\eta})(x_i)\}=L\{(f_0+f_K)(x_i)\}$ and $J(f)=J(f_K+f_\eta)=J(f_K)+J(f_\eta) \ge J(f_K)$.
\end{frame}

\section{Personalized Intervention}
\begin{frame}
\frametitle{Personalized Intervention}	
\end{frame}

\end{document}
